{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Text Analysis\n",
    "\n",
    "Author's name- Antra Tripathi\n",
    "<br> email- tripathiantra074@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Antra\n",
      "[nltk_data]     Tripathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Antra\n",
      "[nltk_data]     Tripathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Input.xlsx')\n",
    "url_id=df['URL_ID']\n",
    "url=df['URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data extraction using beautifulsoup \n",
    "#extracting article and title from given urls using bs4 object; if error while extracting data then storing url_id and index in separate variable\n",
    "# so that we can drop these columns later\n",
    "\n",
    "not_found=[]\n",
    "\n",
    "for i in range(len(url_id)):\n",
    "    link=url[i]\n",
    "    id=url_id[i]\n",
    "    \n",
    "    #requesting url\n",
    "    response=requests.get(link)\n",
    "    \n",
    "    #beautifulsoup object\n",
    "    soup=BeautifulSoup(response.content,'html.parser')\n",
    "    article_name=\"\"\n",
    "    article=\"\"\n",
    "    \n",
    "    #extracting article title\n",
    "    try:\n",
    "        article_name_tag = soup.find('h1', class_='entry-title')\n",
    "        article_name = article_name_tag.text.strip().replace('/', \"\")\n",
    "    except:\n",
    "        not_found.append([id,i])\n",
    "        continue\n",
    "    \n",
    "    #extracting article text\n",
    "    try:\n",
    "        article_content_tag = soup.find('div', class_='td-post-content')\n",
    "        article = article_content_tag.get_text(strip=True, separator=' ')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #Saving title and content with URL_ID as file name\n",
    "    file_name = f\"D:/projects/Web scraping/input_data/{id}.txt\"\n",
    "    with open(file_name,'w',encoding='utf-8') as file:\n",
    "        file.write(article_name + '\\n' + article)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#loading stop words\n",
    "directory = \"D:\\projects\\Web scraping\\StopWords\"\n",
    "\n",
    "# Initialize a set to store unique words\n",
    "all_words = set()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for files in os.listdir(directory):\n",
    "    with open(os.path.join(directory,files),'r',encoding='ISO-8859-1') as f:\n",
    "        words=f.read().splitlines()\n",
    "        all_words.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading positive words\n",
    "directory= \"D:\\projects\\Web scraping\\MasterDictionary\\positive-words.txt\"\n",
    "\n",
    "# Initialize a set to store unique words\n",
    "positive_words=set()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "with open(directory, 'r',encoding='ISO-8859-1') as file:\n",
    "    words=file.read().splitlines()\n",
    "    positive_words.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading negative words\n",
    "directory= r'D:\\projects\\Web scraping\\MasterDictionary'\n",
    "\n",
    "#initialize a set to store negative words\n",
    "negative_words=set()\n",
    "\n",
    "#Loop through each file in the directory\n",
    "for file in os.listdir(directory):\n",
    "  if file =='negative-words.txt':\n",
    "    with open(os.path.join(directory,file),'r',encoding='ISO-8859-1') as f:\n",
    "        negative_words.update(f.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the test into list of tokens using tokenize module\n",
    "# removing the stop words from the text\n",
    " \n",
    "extracted_data=\"D:\\projects\\Web scraping\\input_data\"\n",
    "text=[]\n",
    "for file in os.listdir(extracted_data):\n",
    "  with open(os.path.join(extracted_data,file),'r') as f:\n",
    "    data = f.read()\n",
    "    words=word_tokenize(data)\n",
    "    new= [word for word in words if word.lower() not in all_words]\n",
    "# add each filtered tokens of each file into a list\n",
    "    text.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables for positive and negative words, and polarity and subjectivity score. we will use the converted tokens for calculating these output variables\n",
    "pos_words = []\n",
    "neg_words =[]\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "\n",
    "#calculating positive and negative score for each article\n",
    "for i in range(len(text)):\n",
    "  pos_words.append([word for word in text[i] if word.lower() in positive_words])\n",
    "  neg_words.append([word for word in text[i] if word.lower() in negative_words])\n",
    "  positive_score.append(len(pos_words[i]))\n",
    "  negative_score.append(len(neg_words[i]))\n",
    "  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(text[i])) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing regular expression\n",
    "import re\n",
    "avg_sent_len_list = []\n",
    "percent_complex_words_list = []\n",
    "fog_index_list = []\n",
    "complex_word_count_list = []\n",
    "avg_syllable_word_count_list = []\n",
    "word_count_list = []\n",
    "average_word_length_list = []\n",
    "pronouns=[]\n",
    "\n",
    "\n",
    "#stopwords such as a, an, the etc\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def analyze_text(text):\n",
    "    \n",
    "    #removing punctuations\n",
    "    text=re.sub(r'[^\\w\\s.]','',text)\n",
    "    \n",
    "    #splitting text into sentences\n",
    "    sen=text.split('.')\n",
    "    \n",
    "    #storing number of sentences\n",
    "    num_sentences = len(sen)\n",
    "    \n",
    "    #removing stop words present in text using nltk.corpus.stopwords \n",
    "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "    #calculating average length of words\n",
    "    length = sum(len(word) for word in words)\n",
    "    average_word_length = length / len(words)\n",
    "    word_count=len(words)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    \n",
    "    #counting complex words\n",
    "    complex_words = 0\n",
    "    syllable_count_word=0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for word in words:\n",
    "        x=re.compile('[es|ed$]')\n",
    "        if x.match(word.lower()):\n",
    "            syllable_count_word+=1\n",
    "        else:\n",
    "            for j in word:\n",
    "                if j.lower() in vowels:\n",
    "                    syllable_count_word+=1\n",
    "        if syllable_count_word > 2:\n",
    "            complex_words+=1\n",
    "        syllable_count_word=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    #counting syllables\n",
    "    syllable_count = 0\n",
    "    syllable_words = []\n",
    "    for word in words:\n",
    "        if word.endswith('es'):\n",
    "            word = word[:-2]\n",
    "        elif word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "        vowels = ['a','e','i','o','u']\n",
    "        syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
    "        if syllable_count_word >= 1:\n",
    "            syllable_words.append(word)\n",
    "            syllable_count += syllable_count_word\n",
    "\n",
    "\n",
    "    #counting pronouns\n",
    "    p_list=['i','we','my','ours','us' ]\n",
    "    cnt=0\n",
    "    for word in words:\n",
    "        if word.lower() in p_list:\n",
    "            cnt+=1\n",
    "    \n",
    "    \n",
    "    #counting rest output variables\n",
    "    avg_sent_len = num_words / num_sentences\n",
    "    avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "    percent_complex_words = complex_words / num_words\n",
    "    fog_index = 0.4 * (avg_sent_len + percent_complex_words)\n",
    "    \n",
    "    #returning calculated variables\n",
    "    return avg_sent_len, percent_complex_words*100, fog_index, complex_words, avg_syllable_word_count,word_count,average_word_length,cnt\n",
    "\n",
    "\n",
    "extracted_data=\"D:\\projects\\Web scraping\\input_data\"\n",
    "\n",
    "#iterating through each extracted file present in input_data folder\n",
    "\n",
    "for file in os.listdir(extracted_data):\n",
    "    \n",
    "  with open(os.path.join(extracted_data,file),'r') as f:\n",
    "    text=f.read()\n",
    "    avg_sent_len_val, percent_complex_words_val, fog_index_val, complex_word_count_val, avg_syllable_word_count_val, word_count, average_word_length,p_count= analyze_text(text)\n",
    "    avg_sent_len_list.append(avg_sent_len_val)\n",
    "    percent_complex_words_list.append(percent_complex_words_val)\n",
    "    fog_index_list.append(fog_index_val)\n",
    "    complex_word_count_list.append(complex_word_count_val)\n",
    "    avg_syllable_word_count_list.append(avg_syllable_word_count_val)\n",
    "    word_count_list.append(word_count)\n",
    "    average_word_length_list.append(average_word_length)\n",
    "    pronouns.append(p_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading output file\n",
    "output= pd.read_excel('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns not found\n",
    "index=[]\n",
    "for i in not_found:\n",
    "  index.append(i[1])\n",
    "output.drop(index, inplace=True)\n",
    "excel_file = 'output_file.xlsx'\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "excel_writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "\n",
    "# Write the DataFrame to the Excel file\n",
    "output.to_excel(excel_writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer.save()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Data Structure.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary with all the variables\n",
    "data=[positive_score, negative_score, polarity_score, subjectivity_score, avg_sent_len_list, percent_complex_words_list,fog_index_list, avg_sent_len_list, complex_word_count_list, word_count_list, avg_syllable_word_count_list, pronouns, average_word_length_list]\n",
    "\n",
    "#loading output file\n",
    "df=pd.read_excel('output_file.xlsx')\n",
    "\n",
    "# write the values to the dataframe\n",
    "for i, var in enumerate(data):\n",
    "    df.iloc[:,i+2] = var\n",
    "\n",
    "df.to_excel('Output Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach\n",
    "1. Instead of BeautifulSoup object we can use Scrapy to extract article from urls as spider can follow large amount of links at once.\n",
    "2. It will be beneficial for larger datasets. \n",
    "3. Other approaches like selenium won't work as it will slow down the extracting speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
